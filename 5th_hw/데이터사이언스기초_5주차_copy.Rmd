
# 라이브러리 불러오기
```{r}
# install packages
#install.packages("dplyr")
#install.packages("readr")
#install.packages("tidyr")
#install.packages(("Benchmarking"))
#install.packages("mice")
#install.packages("caret")
#install.packages("rsample")
#install.packages("rpart.plot")

# load library
library(rpart)         # Decision Tree
library(rpart.plot)    # Decision Tree plotting 
library(rsample)      # for 층화 추출
library(caret)
library(dplyr)
library(readr)
library(tidyr)
library(Benchmarking)
library(mice)
library(rpart.plot) # ruleset 추출

```

# 데이터셋 불러오기
```{r}
df <-read.csv("Fish.csv",header = T)
```

# 데이터 전처리
```{r}
##duplicated 함수를 이요한 중복 값 확인
##중복값은 없음을 알 수 있다.
df %>% duplicated()
duplicates <- df %>% duplicated() %>% table()
duplicates

##h distinct()함수를 이용해 중복값을 제거
## 총 17개의 변수 생략
df %>% distinct()

## 결측치 측정 
## 각 컬럼별 결측치는 포함되어 있지 않았다.
col_na <- function(y) {
  return(sum(is.na(y)))
}

na_count <- sapply(df, FUN = col_na)
na_count

##데이터셋의 깨지는 컬럼명을 변경
names(df)[1] <- c("Species")
head(df)

```

# EDA (데이터 탐색)
```{r}

dim(df) #df의 size

print("데이터셋 요약")
summary(df)

print("컬럼별 특징 보기")
str(df) # 컬럼별 특징 보기

```

# Decision Tree - train set과 test set 나누기
```{r}
# loan_status를 기준으로                                               
split <- initial_split(data = df, 0.7, strata = "Species")

# trained data 
train <- training(split)
# test data 
test <- testing(split)

print("### 훈련용 데이터 정보 ###")
summary(train)
str(train)

print("### 테스트용 데이터 정보 ###")
summary(test)
str(test)
```

# Decision Tree 01 - 사전 가지치기 (rpart 파라미터 최적화)

1) minsplit 최적화 - 최적값 minsplit=5
```{r}
check_minspilt <- function(data_csv, i)
{
    train.control<-rpart.control(minsplit = i)
    result <- rpart(Species ~ ., data = df, method="class", control=train.control)
    minspilt_predict <- predict(result, test, type = "class")
    actual <- test$Species
    minspilt_ans <- confusionMatrix(as.factor(actual), minspilt_predict, positive = 'Yes')
    return(minspilt_ans$overall[1])
}

# 실행
minspilt_list <- c()
for(i in 1:100){
  minspilt_list <-c(minspilt_list, check_minspilt(df, i))
}

minspilt_list
which.max(minspilt_list[])
```

2) minbucket 최적화 - 최적값 minbucket = 2
``` {r}
check_minbucket <- function(data_csv, i)
{
    train.control<-rpart.control(minbucket = i)
    result <- rpart(Species ~ ., data = df, method="class", control=train.control)
    minbucket_predict <- predict(result, test, type = "class")
    actual <- test$Species
    minbucket_ans <- confusionMatrix(as.factor(actual), minbucket_predict, positive = 'Yes')
    return(minbucket_ans$overall[1])
}

# 실행
minbucket_list <- c()
for(i in 1:100){
  minbucket_list <-c(minbucket_list, check_minbucket(df, i))
}

minbucket_list
which.max(minbucket_list[])
```

3) maxdepth 최적화 - 최적값 maxdepth = 8
```{r}
check_maxdepth <- function(data_csv, i)
{
    train.control<-rpart.control(maxdepth = i)
    result <- rpart(Species ~ ., data = df, method="class", control=train.control)
    maxdepth_predict <- predict(result, test, type = "class")
    actual <- test$Species
    maxdepth_ans <- confusionMatrix(as.factor(actual), maxdepth_predict, positive = 'Yes')
    return(maxdepth_ans$overall[1])
}

# 실행
maxdepth_list <- c()
for(i in 1:30){
  maxdepth_list <-c(maxdepth_list, check_maxdepth(df, i))
}

maxdepth_list
which.max(maxdepth_list[])
```
- 사전 가지치기 모델 생성
```{r}

train.control<-rpart.control(minsplit=5, minbucket = 2, maxdepth = 8)
pre_tree <- rpart(Species ~ ., data = df, method="class", control=train.control)
minbucket_predict <- predict(pre_tree, test, type = "class")
actual <- test$Species
rpart.plot(pre_tree, box.palette = "pink", roundint = F)
pre_prune_cm <- confusionMatrix(as.factor(actual), minbucket_predict, positive = 'Yes')
pre_prune_cm

```
# Descision Tree 02 - a. Full tree 만들기 (가지치기 x, 최적화 x)
```{r}
# 의사결정 함수 만들기 + 실행하기 + 모델시각화
train_dt <- function(data_csv)
{
  result <- rpart(Species ~ ., data = df, method="class")
  return(result)
}

# 실행

# 시각화
rpart.plot(train_dt(train), roundint = FALSE)

#예측하기
Status_predict <- predict(train_dt(train), test, type = "class")
Status_predict

#평가하기

# 실제 데이터의 대출 상환 여부부
actual <- test$Species
# 비교하고 정확도 계산
confusionMatrix(as.factor(actual), Status_predict, positive = 'Yes')
```

# Descision Tree 02 - b. 사후 가지치기 (cp 최적화)
```{r}
tree <- train_dt(train)
tree$control
tree$cptable
opt <- which.min(tree$cptable[, "xerror"]) #오류률이 제일 적은 CP값 탐색
cp <- tree$cptable[opt, "CP"] #cp 최적화 값
plotcp(tree)
print(cp)
pruned_tree <- prune(tree, cp = cp) #cp =0.01로 가지치기`
rpart.plot(pruned_tree, box.palette = "pink", roundint = F)

# 예측하기
Status_predict <- predict(pruned_tree, test, type = "class")

# 실제 데이터의 정당
actual <- test$Species

# 비교하고 정확도 계산
post_prune_cm <- confusionMatrix(as.factor(actual), Status_predict, positive = 'Yes')
post_prune_cm
```

# 두 예측모델 (사전, 사후 가지치기)의 성능 비교
```{r}

print("사전 가지치기 vs 사후 가지치기 : 정확도")
cat("정확도 (사전):", pre_prune_cm$overall["Accuracy"], "\n정확도 (사후) :", post_prune_cm$overall["Accuracy"], "\n\n")

print("사전 가지치기 vs 사후 가지치기 : 민감도")
cat("민감도 (사전) :", pre_prune_cm$byClass[, "Sensitivity"], "\n민감도 (사후) :", post_prune_cm$byClass[, "Sensitivity"], "\n\n")

print("사전 가지치기 vs 사후 가지치기 : 특이도")
cat("특이도 (사전) :", pre_prune_cm$byClass[, "Specificity"], "\n특이도 (사후) :", post_prune_cm$byClass[, "Specificity"], "\n\n")

```

# Learning Curve를 통해 모델 최적화 확인
```{r}
# 아직 코드 못받음

```

# Rule Set 추출
```{r}

rule <- rpart.rules(x=train_dt(train), cover=TRUE)
# 각 rule에 의해 커버되는 케이스의 비율 가장 큰 것 -> (중요한 rule)
# rule이 적용되는 사례의 예측정확도가 가장 높은 것 -> (신뢰도 높은 rule)
```