#cm <- confusionMatrix(ttesting, vval_var, positive = 'Yes')
#tree <- fancyRpartPlot(fit, type=2, box.palette = "auto")
#print(tree)
#print(cm) # print overview of confusion matrix
## Draw learning curve of DT model (x axis - amount of training data, y axis - accuracy)
data <- data.frame(
Attrition = as.factor(data$Attrition),
OverTime = as.factor(data$OverTime),
MaritalStatus = as.factor(data$MaritalStatus),
TotalWorkingYears = data$TotalWorkingYears,
YearsInCurrentRole = as.factor(data$YearsInCurrentRole)
)
ctrl <- trainControl(classProbs = TRUE, summaryFunction = twoClassSummary)
lda_data <- learning_curve_dat(
dat = data,
outcome = "Attrition",
test_prop = 1/4,
method = "rpart",
metric = "ROC",
trControl = ctrl)
lda_data <- lda_data[!(lda_data$Data == "Resampling"),] # exclude resampling data from learning curve data
p <- ggplot(lda_data, aes(x = Training_Size, y = ROC, color = Data)) + geom_smooth(method = loess, span = .8) + theme_bw()
p
library(caret)
library(rpart)
library(ggplot2)
library(rattle)
## Read dataset
data <- read.csv("/Users/namseoa/Desktop/kau_data+science/hw/4th_hw/WA_Fn-UseC_-HR-Employee-Attrition.csv")
names(data)[1] = c("Age")
## Partitioning
set.seed(1000)
idx <- createDataPartition(y = data$Attrition, p = 0.7, list = FALSE)
training <- data[idx, ]
testing <- data[-idx, ]
## Train model
#fit <- rpart(Attrition~., training, parms=list(split=c("information")), split = )
#fit <- rpart(Attrition~.,
#        training, minsplit=90, minbucket=30,
#        parms = list(split='information'))
#val_var <- predict(fit, newdata=testing, type='class') # apply the model to the test dataset
#ttesting <- as.factor(testing$Attrition)
#vval_var <- as.factor(val_var)
#cm <- confusionMatrix(ttesting, vval_var, positive = 'Yes')
#tree <- fancyRpartPlot(fit, type=2, box.palette = "auto")
#print(tree)
#print(cm) # print overview of confusion matrix
## Draw learning curve of DT model (x axis - amount of training data, y axis - accuracy)
data <- data.frame(
Attrition = as.factor(data$Attrition),
OverTime = as.factor(data$OverTime),
MaritalStatus = as.factor(data$MaritalStatus),
TotalWorkingYears = data$TotalWorkingYears,
YearsInCurrentRole = as.factor(data$YearsInCurrentRole)
)
ctrl <- trainControl(classProbs = TRUE, summaryFunction = twoClassSummary)
lda_data <- learning_curve_dat(
dat = data,
outcome = "Attrition",
test_prop = 1/4,
method = "rpart",
metric = "ROC",
trControl = ctrl,
control=rpart.control(minsplit=90, minbucket=30,
parms=list(split='information')))
lda_data <- lda_data[!(lda_data$Data == "Resampling"),] # exclude resampling data from learning curve data
p <- ggplot(lda_data, aes(x = Training_Size, y = ROC, color = Data)) + geom_smooth(method = loess, span = .8) + theme_bw()
ㅔ
p
# Make pruned-tree, with selected predictor variable set
library(caret)
library(rpart)
library(ggplot2)
library(rattle)
## Load dataset
data <- read.csv("attrition.csv")
names(data)[1] = c("Age")
## Partitioning
set.seed(1000)
idx <- createDataPartition(y = data$Attrition, p = 0.7, list = FALSE)
training <- data[idx, ]
testing <- data[-idx, ]
## Train model
fit <- rpart(Attrition~OverTime + MaritalStatus + TotalWorkingYears + YearsInCurrentRole, minsplit = 90, minbucket = 30, training)
#fit <- rpart(Attrition~OverTime + MaritalStatus + TotalWorkingYears + YearsInCurrentRole, parms = list(split = 'information'), minsplit = 90, minbucket = 30, training)
val_var <- predict(fit, newdata=testing, type='class') # apply the model to the test dataset
ttesting <- as.factor(testing$Attrition)
vval_var <- as.factor(val_var)
cm <- confusionMatrix(ttesting, vval_var, positive = 'Yes')
tree <- fancyRpartPlot(fit, type=2, box.palette = "auto")
print(tree)
print(cm) # print overview of confusion matrix
# Draw learning curve, which applied optimal parameter we found
data3 <- data.frame(
Attrition = as.factor(data$Attrition),
OverTime = as.factor(data$OverTime),
MaritalStatus = as.factor(data$MaritalStatus),
TotalWorkingYears = data$TotalWorkingYears,
YearsInCurrentRole = as.factor(data$YearsInCurrentRole)
)
ctrl <- trainControl(classProbs = TRUE, summaryFunction = twoClassSummary)
lda_data <- learning_curve_dat(
dat = data3,
outcome = "Attrition",
test_prop = 1/4,
method = "rpart",
metric = "ROC",
trControl = ctrl, control = rpart.control(minsplit = 90, minbucket = 30, parms=list(split = 'information')))
lda_data <- lda_data[!(lda_data$Data == "Resampling"),] # exclude resampling data from learning curve data
p <- ggplot(lda_data, aes(x = Training_Size, y = ROC, color = Data)) + geom_smooth(method = loess, span = .8) + theme_bw()
print(p)
# Make pruned-tree, with selected predictor variable set
library(caret)
library(rpart)
library(ggplot2)
library(rattle)
## Load dataset
data <- read.csv("attrition.csv")
names(data)[1] = c("Age")
## Partitioning
set.seed(1000)
idx <- createDataPartition(y = data$Attrition, p = 0.7, list = FALSE)
training <- data[idx, ]
testing <- data[-idx, ]
## Train model
fit <- rpart(Attrition~OverTime + MaritalStatus + TotalWorkingYears + YearsInCurrentRole, minsplit = 90, minbucket = 30, training)
#fit <- rpart(Attrition~OverTime + MaritalStatus + TotalWorkingYears + YearsInCurrentRole, parms = list(split = 'information'), minsplit = 90, minbucket = 30, training)
print(fit)
val_var <- predict(fit, newdata=testing, type='class') # apply the model to the test dataset
ttesting <- as.factor(testing$Attrition)
vval_var <- as.factor(val_var)
cm <- confusionMatrix(ttesting, vval_var, positive = 'Yes')
tree <- fancyRpartPlot(fit, type=2, box.palette = "auto")
print(tree)
print(cm) # print overview of confusion matrix
# Make pruned-tree, with selected predictor variable set
library(caret)
library(rpart)
library(ggplot2)
library(rattle)
## Load dataset
data <- read.csv("attrition.csv")
names(data)[1] = c("Age")
## Partitioning
set.seed(1000)
idx <- createDataPartition(y = data$Attrition, p = 0.7, list = FALSE)
training <- data[idx, ]
testing <- data[-idx, ]
## Train model
fit <- rpart(Attrition~OverTime + MaritalStatus + TotalWorkingYears + YearsInCurrentRole, minsplit = 90, minbucket = 30, training)
#fit <- rpart(Attrition~OverTime + MaritalStatus + TotalWorkingYears + YearsInCurrentRole, parms = list(split = 'information'), minsplit = 90, minbucket = 30, training)
val_var <- predict(fit, newdata=testing, type='class') # apply the model to the test dataset
print(val_var)
ttesting <- as.factor(testing$Attrition)
vval_var <- as.factor(val_var)
cm <- confusionMatrix(ttesting, vval_var, positive = 'Yes')
tree <- fancyRpartPlot(fit, type=2, box.palette = "auto")
print(tree)
print(cm) # print overview of confusion matrix
# Make pruned-tree, with selected predictor variable set
library(caret)
library(rpart)
library(ggplot2)
library(rattle)
## Load dataset
data <- read.csv("attrition.csv")
names(data)[1] = c("Age")
## Partitioning
set.seed(1000)
idx <- createDataPartition(y = data$Attrition, p = 0.7, list = FALSE)
training <- data[idx, ]
testing <- data[-idx, ]
## Train model
fit <- rpart(Attrition~OverTime + MaritalStatus + TotalWorkingYears + YearsInCurrentRole, minsplit = 90, minbucket = 30, training)
#fit <- rpart(Attrition~OverTime + MaritalStatus + TotalWorkingYears + YearsInCurrentRole, parms = list(split = 'information'), minsplit = 90, minbucket = 30, training)
val_var <- predict(fit, newdata=testing, type='class') # apply the model to the test dataset
print()
# Make pruned-tree, with selected predictor variable set
library(caret)
library(rpart)
library(ggplot2)
library(rattle)
## Load dataset
data <- read.csv("attrition.csv")
names(data)[1] = c("Age")
## Partitioning
set.seed(1000)
idx <- createDataPartition(y = data$Attrition, p = 0.7, list = FALSE)
training <- data[idx, ]
testing <- data[-idx, ]
## Train model
fit <- rpart(Attrition~OverTime + MaritalStatus + TotalWorkingYears + YearsInCurrentRole, minsplit = 90, minbucket = 30, training)
#fit <- rpart(Attrition~OverTime + MaritalStatus + TotalWorkingYears + YearsInCurrentRole, parms = list(split = 'information'), minsplit = 90, minbucket = 30, training)
val_var <- predict(fit, newdata=testing, type='class') # apply the model to the test dataset
cat("\n\n\n")
print(val_var)
ttesting <- as.factor(testing$Attrition)
vval_var <- as.factor(val_var)
cm <- confusionMatrix(ttesting, vval_var, positive = 'Yes')
tree <- fancyRpartPlot(fit, type=2, box.palette = "auto")
print(tree)
print(cm) # print overview of confusion matrix
# Make pruned-tree, with selected predictor variable set
library(caret)
library(rpart)
library(ggplot2)
library(rattle)
## Load dataset
data <- read.csv("attrition.csv")
names(data)[1] = c("Age")
## Partitioning
set.seed(1000)
idx <- createDataPartition(y = data$Attrition, p = 0.7, list = FALSE)
training <- data[idx, ]
testing <- data[-idx, ]
## Train model
fit <- rpart(Attrition~OverTime + MaritalStatus + TotalWorkingYears + YearsInCurrentRole, minsplit = 90, minbucket = 30, training)
#fit <- rpart(Attrition~OverTime + MaritalStatus + TotalWorkingYears + YearsInCurrentRole, parms = list(split = 'information'), minsplit = 90, minbucket = 30, training)
val_var <- predict(fit, newdata=testing, type='class') # apply the model to the test dataset
cat("\n\n\n")
#print(val_var)
ttesting <- as.factor(testing$Attrition)
vval_var <- as.factor(val_var)
cm <- confusionMatrix(ttesting, vval_var, positive = 'Yes')
tree <- fancyRpartPlot(fit, type=2, box.palette = "auto")
print(tree)
print(cm) # print overview of confusion matrix
#install packages
install.packages("dplyr")
install.packages("readr")
#install packages
install.packages("dplyr")
install.packages("readr")
df <-read.csv("Fish.csv",header = T)
##duplicated 함수를 이요한 중복 값 확인
##중복값은 없음을 알 수 있다.
df %>% duplicated()
#install packages
install.packages("dplyr")
install.packages("readr")
#install packages
# load library
library(rpart)         # Decision Tree
library(rpart.plot)    # Decision Tree plotting
library(rsample)      # for 층화 추출
library(caret)
library(dplyr)
library(readr)
library(ggplot2)
library(MLmetrics)
library(tidyr)
library(Benchmarking)
library(mice)
library(rpart.plot) # ruleset 추출
library(dplyr) # 피어슨 상관계수
df <-read.csv("Fish.csv",header = T)
##duplicated 함수를 이요한 중복 값 확인
##중복값은 없음을 알 수 있다.
df %>% duplicated()
duplicates <- df %>% duplicated() %>% table()
duplicates
##h distinct()함수를 이용해 중복값을 제거
## 총 17개의 변수 생략
df %>% distinct()
## 결측치 측정
## 각 컬럼별 결측치는 포함되어 있지 않았다.
col_na <- function(y) {
return(sum(is.na(y)))
}
na_count <- sapply(df, FUN = col_na)
na_count
##데이터셋의 깨지는 컬럼명을 변경
names(df)[1] <- c("Species")
head(df)
dim(df) #df의 size
print("데이터셋 요약")
summary(df)
print("컬럼별 특징 보기")
str(df) # 컬럼별 특징 보기
#데이터 탐색 - 기술적 통계 (피어슨 상관계수)
cor(df %>% select(-Species), use = "all.obs", method = "pearson")
dim(df) #df의 size
print("데이터셋 요약")
summary(df)
print("컬럼별 특징 보기")
str(df) # 컬럼별 특징 보기
#데이터 탐색 - 기술적 통계 (피어슨 상관계수)
cor(df %>% select(-Species), use = "all.obs", method = "pearson")
# loan_status를 기준으로
set.seed(1000)
split <- initial_split(data = df, 0.7, strata = "Species")
# trained data
train <- training(split)
# test data
test <- testing(split)
print("### 훈련용 데이터 정보 ###")
summary(train)
str(train)
print("### 테스트용 데이터 정보 ###")
summary(test)
str(test)
## 사전 가지치기 파라미터 최적화
check_all <- function(df, ms, mb, md){
train.control<-rpart.control(minsplit = ms, minbucket = mb, maxdepth=md)
tmp_tree <- rpart(Species ~ ., data = df, method="class", control=train.control)
pre_tree <- predict(tmp_tree, test, type='class')
accuracy = confusionMatrix(as.factor(test$Species), pre_tree)$overall[1]
row <- c(ms, mb, md, accuracy)
return(row)
}
check_list <- c()
for (k in 1:10){
for(j in 2:10){
for(i in 1:10){
check_list<-rbind(check_list, check_all(df, i, j, k))
}
}
}
colnames(check_list) <- c("minsplit", "minbucket", "maxdepth", "Accuracy")
print(check_list[check_list[, 4] > 0.74, ])
# 모델 생성
train.control<-rpart.control(minsplit=1, minbucket = 2, maxdepth = 6)
pre_tree <- rpart(Species ~ ., data = df, method="class", control=train.control)
printcp(pre_tree)
minbucket_predict <- predict(pre_tree, test, type = "class")
actual <- test$Species
rpart.plot(pre_tree, box.palette = "auto", roundint = F)
pre_prune_cm <- confusionMatrix(as.factor(actual), minbucket_predict, positive = 'Yes')
pre_prune_cm
# 모델 생성
train.control<-rpart.control(minsplit=1, minbucket = 2, maxdepth = 6)
pre_tree <- rpart(Species ~ ., data = df, method="class", control=train.control)
printcp(pre_tree)
minbucket_predict <- predict(pre_tree, test, type = "class")
actual <- test$Species
rpart.plot(pre_tree, box.palette = "auto", roundint = F)
pre_prune_cm <- confusionMatrix(as.factor(actual), minbucket_predict, positive = 'Yes')
pre_prune_cm
##full tree  만들기
train.control<-rpart.control(minsplit = 1, minbucket = 2)
full_tree <- rpart(Species ~ .,
data = train,
method = "class",
cp = -1,
control=train.control
)
rpart.plot(pruned_tree, box.palette = "auto", roundint = FALSE)
##full tree  만들기
train.control<-rpart.control(minsplit = 1, minbucket = 2)
full_tree <- rpart(Species ~ .,
data = train,
method = "class",
cp = -1,
control=train.control
)
rpart.plot(full_tree, box.palette = "auto", roundint = FALSE)
full_tree$control
full_tree$cptable
opt <- which.min(full_tree$cptable[, "xerror"]) #오류률이 제일 적은 CP값 탐색
cp <- full_tree$cptable[opt, "CP"] #cp 최적화 값
plotcp(full_tree) # cp 최적화 플롯 출력
print(cp)
##full tree  만들기
train.control<-rpart.control(minsplit = 1, minbucket = 2)
full_tree <- rpart(Species ~ .,
data = train,
method = "class",
cp = -1,
control=train.control
)
rpart.plot(full_tree, box.palette = "auto", roundint = FALSE)
full_tree$control
full_tree$cptable
opt <- which.min(full_tree$cptable[, "xerror"]) #오류률이 제일 적은 CP값 탐색
cp <- full_tree$cptable[opt, "CP"] #cp 최적화 값
plotcp(full_tree) # cp 최적화 플롯 출력
print(cp)
##full tree  만들기
train.control<-rpart.control(minsplit = 1, minbucket = 2)
full_tree <- rpart(Species ~ .,
data = train,
method = "class",
cp = -1,
control=train.control
)
rpart.plot(full_tree, box.palette = "auto", roundint = FALSE)
full_tree$control
full_tree$cptable
opt <- which.min(full_tree$cptable[, "xerror"]) #오류률이 제일 적은 CP값 탐색
cp <- full_tree$cptable[opt, "CP"] #cp 최적화 값
plotcp(full_tree) # cp 최적화 플롯 출력
print(cp)
#install packages
# load library
library(rpart)         # Decision Tree
library(rpart.plot)    # Decision Tree plotting
library(rsample)      # for 층화 추출
library(caret)
library(dplyr)
library(readr)
library(ggplot2)
library(MLmetrics)
library(tidyr)
library(Benchmarking)
library(mice)
library(rpart.plot) # ruleset 추출
library(dplyr) # 피어슨 상관계수
df <-read.csv("Fish.csv",header = T)
##duplicated 함수를 이요한 중복 값 확인
##중복값은 없음을 알 수 있다.
df %>% duplicated()
duplicates <- df %>% duplicated() %>% table()
duplicates
##h distinct()함수를 이용해 중복값을 제거
## 총 17개의 변수 생략
df %>% distinct()
## 결측치 측정
## 각 컬럼별 결측치는 포함되어 있지 않았다.
col_na <- function(y) {
return(sum(is.na(y)))
}
na_count <- sapply(df, FUN = col_na)
na_count
##데이터셋의 깨지는 컬럼명을 변경
names(df)[1] <- c("Species")
head(df)
dim(df) #df의 size
print("데이터셋 요약")
summary(df)
print("컬럼별 특징 보기")
str(df) # 컬럼별 특징 보기
#데이터 탐색 - 기술적 통계 (피어슨 상관계수)
cor(df %>% select(-Species), use = "all.obs", method = "pearson")
# loan_status를 기준으로
set.seed(1000)
split <- initial_split(data = df, 0.7, strata = "Species")
# trained data
train <- training(split)
# test data
test <- testing(split)
print("### 훈련용 데이터 정보 ###")
summary(train)
str(train)
print("### 테스트용 데이터 정보 ###")
summary(test)
str(test)
## 사전 가지치기 파라미터 최적화
check_all <- function(df, ms, mb, md){
train.control<-rpart.control(minsplit = ms, minbucket = mb, maxdepth=md)
tmp_tree <- rpart(Species ~ ., data = df, method="class", control=train.control)
pre_tree <- predict(tmp_tree, test, type='class')
accuracy = confusionMatrix(as.factor(test$Species), pre_tree)$overall[1]
row <- c(ms, mb, md, accuracy)
return(row)
}
check_list <- c()
for (k in 1:10){
for(j in 2:10){
for(i in 1:10){
check_list<-rbind(check_list, check_all(df, i, j, k))
}
}
}
colnames(check_list) <- c("minsplit", "minbucket", "maxdepth", "Accuracy")
print(check_list[check_list[, 4] > 0.74, ])
# 모델 생성
train.control<-rpart.control(minsplit=1, minbucket = 2, maxdepth = 6)
pre_tree <- rpart(Species ~ ., data = df, method="class", control=train.control)
printcp(pre_tree)
minbucket_predict <- predict(pre_tree, test, type = "class")
actual <- test$Species
rpart.plot(pre_tree, box.palette = "auto", roundint = F)
pre_prune_cm <- confusionMatrix(as.factor(actual), minbucket_predict, positive = 'Yes')
pre_prune_cm
##full tree  만들기
train.control<-rpart.control(minsplit = 1, minbucket = 2)
full_tree <- rpart(Species ~ .,
data = train,
method = "class",
cp = -1,
control=train.control
)
rpart.plot(pruned_tree, box.palette = "auto", roundint = FALSE)
pruned_tree <- prune(tree, cp = 0.046) # cp = 0.046로 가지치기`
pruned_tree <- prune(tree, cp = 0.046) # cp = 0.046로 가지치기`
##full tree  만들기
train.control<-rpart.control(minsplit = 1, minbucket = 2)
full_tree <- rpart(Species ~ .,
data = train,
method = "class",
cp = -1,
control=train.control
)
#rpart.plot(pruned_tree, box.palette = "auto", roundint = FALSE)
full_tree$control
full_tree$cptable
opt <- which.min(full_tree$cptable[, "xerror"]) #오류률이 제일 적은 CP값 탐색
cp <- full_tree$cptable[opt, "CP"] #cp 최적화 값
plotcp(full_tree) # cp 최적화 플롯 출력
print(cp)
pruned_tree <- prune(tree, cp = 0.046) # cp = 0.046로 가지치기`
pruned_tree <- prune(tree, cp = 0.046) # cp = 0.046로 가지치기`
pruned_tree <- prune(tree, cp = 0.046) # cp = 0.046로 가지치기`
pruned_tree <- prune(full_tree, cp = 0.046) # cp = 0.046로 가지치기`
rpart.plot(pruned_tree, box.palette = "auto", roundint = F)
# 예측하기
Status_predict <- predict(pruned_tree, test, type = "class")
# 실제 데이터의 정당
actual <- test$Species
# 비교하고 정확도 계산
post_prune_cm <- confusionMatrix(as.factor(actual), Status_predict, positive = 'Yes')
post_prune_cm
